{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "# import matplotlib.pyplot as plt\n",
    "import tqdm.notebook as tqdm\n",
    "import random\n",
    "from itertools import cycle\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import tqdm.notebook as tqdm\n",
    "# from peft import PeftModel, PeftConfig, get_peft_model\n",
    "# from datasets import load_dataset\n",
    "# import wandb\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "load_dotenv()\n",
    "hf_access_token = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from dataclasses import asdict\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "exp_time = datetime.now().strftime(\"%b%d\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"THUDM/cogvlm2-llama3-chat-19B-int4\"\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "TORCH_TYPE = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability()[\n",
    "    0] >= 8 else torch.float16\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=TORCH_TYPE,\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch, infer_auto_device_map\n",
    "\n",
    "MODEL_PATH = \"THUDM/cogvlm2-llama3-chat-19B\"\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "TORCH_TYPE = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability()[\n",
    "    0] >= 8 else torch.float16\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "with init_empty_weights():\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        torch_dtype=TORCH_TYPE,\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "\n",
    "device_map = infer_auto_device_map(\n",
    "    model=model,\n",
    "    max_memory={i: \"23GiB\" for i in range(torch.cuda.device_count())},\n",
    "    # set 23GiB for each GPU, depends on your GPU memory, you can adjust this value\n",
    "    no_split_module_classes=[\"CogVLMDecoderLayer\"]\n",
    ")\n",
    "\n",
    "save_dir = \"/root/multimodal-eng/models/3jun-int4\"\n",
    "model.save_pretrained(save_dir)\n",
    "checkpoint = \"/root/multimodal-eng/models/3jun-int4\"\n",
    "# model = load_checkpoint_and_dispatch(model, MODEL_PATH, device_map=device_map, dtype=TORCH_TYPE)\n",
    "model = load_checkpoint_and_dispatch(model, checkpoint, device_map=device_map, dtype=TORCH_TYPE)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_PATH = \"THUDM/cogvlm2-llama3-chat-19B\"\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "TORCH_TYPE = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8 else torch.float16\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=TORCH_TYPE,\n",
    "    trust_remote_code=True,\n",
    ").to(DEVICE).eval()\n",
    "\n",
    "text_only_template = \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: {} ASSISTANT:\"\n",
    "\n",
    "while True:\n",
    "    image_path = input(\"image path >>>>> \")\n",
    "    if image_path == '':\n",
    "        print('You did not enter image path, the following will be a plain text conversation.')\n",
    "        image = None\n",
    "        text_only_first_query = True\n",
    "    else:\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "    history = []\n",
    "\n",
    "    while True:\n",
    "        query = input(\"Human:\")\n",
    "        if query == \"clear\":\n",
    "            break\n",
    "\n",
    "        if image is None:\n",
    "            if text_only_first_query:\n",
    "                query = text_only_template.format(query)\n",
    "                text_only_first_query = False\n",
    "            else:\n",
    "                old_prompt = ''\n",
    "                for _, (old_query, response) in enumerate(history):\n",
    "                    old_prompt += old_query + \" \" + response + \"\\n\"\n",
    "                query = old_prompt + \"USER: {} ASSISTANT:\".format(query)\n",
    "        if image is None:\n",
    "            input_by_model = model.build_conversation_input_ids(\n",
    "                tokenizer,\n",
    "                query=query,\n",
    "                history=history,\n",
    "                template_version='chat'\n",
    "            )\n",
    "        else:\n",
    "            input_by_model = model.build_conversation_input_ids(\n",
    "                tokenizer,\n",
    "                query=query,\n",
    "                history=history,\n",
    "                images=[image],\n",
    "                template_version='chat'\n",
    "            )\n",
    "        inputs = {\n",
    "            'input_ids': input_by_model['input_ids'].unsqueeze(0).to(DEVICE),\n",
    "            'token_type_ids': input_by_model['token_type_ids'].unsqueeze(0).to(DEVICE),\n",
    "            'attention_mask': input_by_model['attention_mask'].unsqueeze(0).to(DEVICE),\n",
    "            'images': [[input_by_model['images'][0].to(DEVICE).to(TORCH_TYPE)]] if image is not None else None,\n",
    "        }\n",
    "        gen_kwargs = {\n",
    "            \"max_new_tokens\": 2048,\n",
    "            \"pad_token_id\": 128002,  \n",
    "        }\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, **gen_kwargs)\n",
    "            outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "            response = tokenizer.decode(outputs[0])\n",
    "            response = response.split(\"<|end_of_text|>\")[0]\n",
    "            print(\"\\nCogVLM2:\", response)\n",
    "        history.append((query, response))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# getting batched inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is a demo for using CogVLM2 in CLI using multi-GPU with lower memory.\n",
    "If your single GPU is not enough to drive this model, you can use this demo to run this model on multiple graphics cards with limited video memory.\n",
    "Here, we default that your graphics card has 24GB of video memory, which is not enough to load the FP16 / BF16 model.\n",
    "so , need to use two graphics cards to load. We set '23GiB' for each GPU to avoid out of memory.\n",
    "\"\"\"\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch, infer_auto_device_map\n",
    "import os\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from tqdm import tqdm\n",
    "import textwrap\n",
    "\n",
    "MODEL_PATH = \"THUDM/cogvlm2-llama3-chat-19B\"\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "TORCH_TYPE = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability()[\n",
    "    0] >= 8 else torch.float16\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "with init_empty_weights():\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        torch_dtype=TORCH_TYPE,\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "\n",
    "device_map = infer_auto_device_map(\n",
    "    model=model,\n",
    "    max_memory={i: \"23GiB\" for i in range(torch.cuda.device_count())},\n",
    "    # set 23GiB for each GPU, depends on your GPU memory, you can adjust this value\n",
    "    no_split_module_classes=[\"CogVLMDecoderLayer\"]\n",
    ")\n",
    "checkpoint = \"/root/multimodal-eng/models/28may-1\"\n",
    "model = load_checkpoint_and_dispatch(model, checkpoint, device_map=device_map, dtype=TORCH_TYPE)\n",
    "model = model.eval()\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "text_only_template = \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: {} ASSISTANT:\"\n",
    "\n",
    "image_folder = \"/root/multimodal-eng/pics_of_mates_faces/\"\n",
    "user_input = \"Describe this person to me.\"\n",
    "\n",
    "images_and_responses = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_inputs = []\n",
    "\n",
    "for idx, image_file in tqdm(enumerate(os.listdir(image_folder))):\n",
    "    if not image_file.endswith(\".png\"):\n",
    "        continue\n",
    "    image_path = os.path.join(image_folder, image_file)\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "    history = []\n",
    "\n",
    "    input_by_model = model.build_conversation_input_ids(\n",
    "        tokenizer,\n",
    "        query=user_input,\n",
    "        images=[image],\n",
    "        template_version='chat'\n",
    "    )\n",
    "    inputs = {\n",
    "        'input_ids': input_by_model['input_ids'].unsqueeze(0).to(DEVICE),\n",
    "        'token_type_ids': input_by_model['token_type_ids'].unsqueeze(0).to(DEVICE),\n",
    "        'attention_mask': input_by_model['attention_mask'].unsqueeze(0).to(DEVICE),\n",
    "        'images': [[input_by_model['images'][0].to(DEVICE).to(TORCH_TYPE)]] if image is not None else None,\n",
    "    }\n",
    "\n",
    "    batched_inputs.append(inputs)\n",
    "\n",
    "batched_input_ids = torch.cat([inputs['input_ids'] for inputs in batched_inputs], dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids=batched_input_ids[:4, :],\n",
    "        token_type_ids=torch.cat([inputs['token_type_ids'] for inputs in batched_inputs[:4]], dim=0),\n",
    "        max_new_tokens=2048,\n",
    "        pad_token_id=128002,\n",
    "        images=[inputs['images'][0] for inputs in batched_inputs[:4]]\n",
    "    )\n",
    "for output_idx, outputs in enumerate(outputs):\n",
    "    response = tokenizer.decode(outputs)\n",
    "    response = response.split(\"<|end_of_text|>\")[0].split(\"Question:\")[-1]\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jsonformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jsonformer.format import highlight_values\n",
    "from jsonformer.main import Jsonformer\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(\"Loading model and tokenizer...\")\n",
    "model_name = \"databricks/dolly-v2-3b\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, use_cache=True, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, use_cache=True)\n",
    "print(\"Loaded model and tokenizer\")\n",
    "\n",
    "car = {\n",
    "  \"type\": \"object\",\n",
    "  \"properties\": {\n",
    "    \"car\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"make\": {\"type\": \"string\"},\n",
    "        \"model\": {\"type\": \"string\"},\n",
    "        \"year\": {\"type\": \"number\"},\n",
    "        \"colors\": {\n",
    "          \"type\": \"array\",\n",
    "          \"items\": {\"type\": \"string\"}\n",
    "        },\n",
    "        \"features\": {\n",
    "          \"type\": \"object\",\n",
    "          \"properties\": {\n",
    "            \"audio\": {\n",
    "              \"type\": \"object\",\n",
    "              \"properties\": {\n",
    "                \"brand\": {\"type\": \"string\"},\n",
    "                \"speakers\": {\"type\": \"number\"},\n",
    "                \"hasBluetooth\": {\"type\": \"boolean\"}\n",
    "              }\n",
    "            },\n",
    "            \"safety\": {\n",
    "              \"type\": \"object\",\n",
    "              \"properties\": {\n",
    "                \"airbags\": {\"type\": \"number\"},\n",
    "                \"parkingSensors\": {\"type\": \"boolean\"},\n",
    "                \"laneAssist\": {\"type\": \"boolean\"}\n",
    "              }\n",
    "            },\n",
    "            \"performance\": {\n",
    "              \"type\": \"object\",\n",
    "              \"properties\": {\n",
    "                \"engine\": {\"type\": \"string\"},\n",
    "                \"horsepower\": {\"type\": \"number\"},\n",
    "                \"topSpeed\": {\"type\": \"number\"}\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    \"owner\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"firstName\": {\"type\": \"string\"},\n",
    "        \"lastName\": {\"type\": \"string\"},\n",
    "        \"age\": {\"type\": \"number\"},\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "builder = Jsonformer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    json_schema=car,\n",
    "    prompt=\"Generate an example car\",\n",
    ")\n",
    "\n",
    "print(\"Generating...\")\n",
    "output = builder()\n",
    "\n",
    "highlight_values(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.paddding_side = \"left\"\n",
    "\n",
    "while True:\n",
    "    text_input = input(\"Enter a question: \")\n",
    "    input1 = \"Question: \" + text_input + \"Short Answer:\"\n",
    "    input2 = \"Question: \" + text_input + \"Long Answer:\"\n",
    "    if text_input == \"exit\":\n",
    "        break\n",
    "    batch_tokens = tokenizer([input1, input2], return_tensors=\"pt\")\n",
    "    out = model.generate(**batch_tokens)\n",
    "    print(f\"Short Answer: {tokenizer.decode(out[0])}\")\n",
    "    print(f\"Long Answer: {tokenizer.decode(out[1])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is a demo for using CogVLM2 in CLI using multi-GPU with lower memory.\n",
    "If your single GPU is not enough to drive this model, you can use this demo to run this model on multiple graphics cards with limited video memory.\n",
    "Here, we default that your graphics card has 24GB of video memory, which is not enough to load the FP16 / BF16 model.\n",
    "so , need to use two graphics cards to load. We set '23GiB' for each GPU to avoid out of memory.\n",
    "\"\"\"\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch, infer_auto_device_map\n",
    "import os\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from tqdm import tqdm\n",
    "import textwrap\n",
    "\n",
    "MODEL_PATH = \"THUDM/cogvlm2-llama3-chat-19B\"\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "TORCH_TYPE = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability()[\n",
    "    0] >= 8 else torch.float16\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "with init_empty_weights():\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        torch_dtype=TORCH_TYPE,\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "\n",
    "device_map = infer_auto_device_map(\n",
    "    model=model,\n",
    "    max_memory={i: \"23GiB\" for i in range(torch.cuda.device_count())},\n",
    "    # set 23GiB for each GPU, depends on your GPU memory, you can adjust this value\n",
    "    no_split_module_classes=[\"CogVLMDecoderLayer\"]\n",
    ")\n",
    "checkpoint = \"/root/multimodal-eng/models/28may-1\"\n",
    "model = load_checkpoint_and_dispatch(model, checkpoint, device_map=device_map, dtype=TORCH_TYPE)\n",
    "model = model.eval()\n",
    "print(\"Model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "car = {\n",
    "  \"type\": \"object\",\n",
    "  \"properties\": {\n",
    "    \"car\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"make\": {\"type\": \"string\"},\n",
    "        \"model\": {\"type\": \"string\"},\n",
    "        \"year\": {\"type\": \"number\"},\n",
    "        \"colors\": {\n",
    "          \"type\": \"array\",\n",
    "          \"items\": {\"type\": \"string\"}\n",
    "        },\n",
    "        \"features\": {\n",
    "          \"type\": \"object\",\n",
    "          \"properties\": {\n",
    "            \"audio\": {\n",
    "              \"type\": \"object\",\n",
    "              \"properties\": {\n",
    "                \"brand\": {\"type\": \"string\"},\n",
    "                \"speakers\": {\"type\": \"number\"},\n",
    "                \"hasBluetooth\": {\"type\": \"boolean\"}\n",
    "              }\n",
    "            },\n",
    "            \"safety\": {\n",
    "              \"type\": \"object\",\n",
    "              \"properties\": {\n",
    "                \"airbags\": {\"type\": \"number\"},\n",
    "                \"parkingSensors\": {\"type\": \"boolean\"},\n",
    "                \"laneAssist\": {\"type\": \"boolean\"}\n",
    "              }\n",
    "            },\n",
    "            \"performance\": {\n",
    "              \"type\": \"object\",\n",
    "              \"properties\": {\n",
    "                \"engine\": {\"type\": \"string\"},\n",
    "                \"horsepower\": {\"type\": \"number\"},\n",
    "                \"topSpeed\": {\"type\": \"number\"}\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    \"owner\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"firstName\": {\"type\": \"string\"},\n",
    "        \"lastName\": {\"type\": \"string\"},\n",
    "        \"age\": {\"type\": \"number\"},\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "builder = Jsonformer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    json_schema=car,\n",
    "    prompt=\"Generate an example car\",\n",
    ")\n",
    "\n",
    "print(\"Generating...\")\n",
    "output = builder()\n",
    "\n",
    "highlight_values(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CogVLMJsonformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.venv/multi/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This is a demo for using CogVLM2 in CLI using multi-GPU with lower memory.\n",
    "If your single GPU is not enough to drive this model, you can use this demo to run this model on multiple graphics cards with limited video memory.\n",
    "Here, we default that your graphics card has 24GB of video memory, which is not enough to load the FP16 / BF16 model.\n",
    "so , need to use two graphics cards to load. We set '23GiB' for each GPU to avoid out of memory.\n",
    "\"\"\"\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch, infer_auto_device_map\n",
    "import os\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from tqdm import tqdm\n",
    "import textwrap\n",
    "from jsonformer.format import highlight_values\n",
    "from jsonformer.main import Jsonformer\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from subclass_jsonformer import CogVLMJsonformer\n",
    "\n",
    "\n",
    "\n",
    "car = {\n",
    "  \"type\": \"object\",\n",
    "  \"properties\": {\n",
    "    \"car\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"make\": {\"type\": \"string\"},\n",
    "        \"model\": {\"type\": \"string\"},\n",
    "        \"year\": {\"type\": \"number\"},\n",
    "        \"colors\": {\n",
    "          \"type\": \"array\",\n",
    "          \"items\": {\"type\": \"string\"}\n",
    "        },\n",
    "        \"features\": {\n",
    "          \"type\": \"object\",\n",
    "          \"properties\": {\n",
    "            \"audio\": {\n",
    "              \"type\": \"object\",\n",
    "              \"properties\": {\n",
    "                \"brand\": {\"type\": \"string\"},\n",
    "                \"speakers\": {\"type\": \"number\"},\n",
    "                \"hasBluetooth\": {\"type\": \"boolean\"}\n",
    "              }\n",
    "            },\n",
    "            \"safety\": {\n",
    "              \"type\": \"object\",\n",
    "              \"properties\": {\n",
    "                \"airbags\": {\"type\": \"number\"},\n",
    "                \"parkingSensors\": {\"type\": \"boolean\"},\n",
    "                \"laneAssist\": {\"type\": \"boolean\"}\n",
    "              }\n",
    "            },\n",
    "            \"performance\": {\n",
    "              \"type\": \"object\",\n",
    "              \"properties\": {\n",
    "                \"engine\": {\"type\": \"string\"},\n",
    "                \"horsepower\": {\"type\": \"number\"},\n",
    "                \"topSpeed\": {\"type\": \"number\"}\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    \"owner\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"firstName\": {\"type\": \"string\"},\n",
    "        \"lastName\": {\"type\": \"string\"},\n",
    "        \"age\": {\"type\": \"number\"},\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "MODEL_PATH = \"THUDM/cogvlm2-llama3-chat-19B\"\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "TORCH_TYPE = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability()[\n",
    "    0] >= 8 else torch.float16\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "with init_empty_weights():\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        torch_dtype=TORCH_TYPE,\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "\n",
    "device_map = infer_auto_device_map(\n",
    "    model=model,\n",
    "    max_memory={i: \"23GiB\" for i in range(torch.cuda.device_count())},\n",
    "    # set 23GiB for each GPU, depends on your GPU memory, you can adjust this value\n",
    "    no_split_module_classes=[\"CogVLMDecoderLayer\"]\n",
    ")\n",
    "checkpoint = \"/root/multimodal-eng/models/28may-1\"\n",
    "model = load_checkpoint_and_dispatch(model, checkpoint, device_map=device_map, dtype=TORCH_TYPE)\n",
    "model = model.eval()\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "\n",
    "# builder = Jsonformer(\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"THUDM/cogvlm2-llama3-chat-19B-int4\"\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "TORCH_TYPE = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability()[\n",
    "    0] >= 8 else torch.float16\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer.padding_side = \"left\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=TORCH_TYPE,\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from PIL import Image\n",
    "image = Image.open(\"/root/multimodal-eng/pics_of_mates_faces/Screenshot_2020_04_06_at_151459__.png\").convert('RGB')\n",
    "\n",
    "builder = CogVLMJsonformer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    json_schema=car,\n",
    "    prompt=\"Generate an example car\",\n",
    "    images=[image],\n",
    "    debug=True,\n",
    ")\n",
    "\n",
    "print(\"Generating...\")\n",
    "output = builder()\n",
    "\n",
    "highlight_values(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.venv/multi/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/root/.venv/multi/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "prompt = \"- problems: numbers suck, speed is slow, need batching with images and language stuff, OOMs for image inputs with the 4 bit model, but in principle I can run inference on this model with one gpu, maybe I should debug with this model on the 80gb a100, except even on an a100 I get errors! What happens when I only run one inference step with an image text pair, I might as well get super familiar with how to do this. \"*10\n",
    "prompt12 = \"Describe this person in a poem\"\n",
    "prompt_empty = \"\"\n",
    "\n",
    "image = Image.open(\"/root/multimodal-eng/pics_of_mates_faces/Screenshot_2020_04_06_at_151459__.png\").convert('RGB')\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "empty_build_input_ids = model.build_conversation_input_ids(\n",
    "    tokenizer=tokenizer,\n",
    "    query=prompt_empty,\n",
    "    images=[image] if image is not None else None,\n",
    "    template_version=\"base\",\n",
    ")\n",
    "prompt_build_input_ids = model.build_conversation_input_ids(\n",
    "    tokenizer=tokenizer,\n",
    "    query=prompt_empty + prompt12,\n",
    "    images=[image] if image is not None else None,\n",
    "    template_version=\"base\",\n",
    ")\n",
    "input_tokens = empty_build_input_ids['input_ids'].to(model.device)\n",
    "image_only_inputs1 = {\n",
    "    'input_ids': empty_build_input_ids['input_ids'].unsqueeze(0).to(model.device),\n",
    "    'token_type_ids': empty_build_input_ids['token_type_ids'].unsqueeze(0).to(model.device),\n",
    "    'attention_mask': empty_build_input_ids['attention_mask'].unsqueeze(0).to(model.device),\n",
    "    'images': [[empty_build_input_ids['images'][0].to(model.device).to(torch.bfloat16)]] if image is not None else None,\n",
    "}\n",
    "image_only_inputs2 = {\n",
    "    \"input_ids\": torch.stack([empty_build_input_ids['input_ids'], empty_build_input_ids['input_ids']], dim=0).to(model.device),\n",
    "    \"token_type_ids\": torch.stack([empty_build_input_ids['token_type_ids'], empty_build_input_ids['token_type_ids']], dim=0).to(model.device),\n",
    "    \"attention_mask\": torch.stack([empty_build_input_ids['attention_mask'], empty_build_input_ids['attention_mask']], dim=0).to(model.device),\n",
    "    \"images\": [[empty_build_input_ids['images'][0].to(model.device).to(torch.bfloat16)]]*2 if image is not None else None,\n",
    "}\n",
    "prompt_only_inputs1 = {\n",
    "    'input_ids': prompt_build_input_ids['input_ids'].unsqueeze(0).to(model.device),\n",
    "    'token_type_ids': prompt_build_input_ids['token_type_ids'].unsqueeze(0).to(model.device),\n",
    "    'attention_mask': prompt_build_input_ids['attention_mask'].unsqueeze(0).to(model.device),\n",
    "    'images': [[prompt_build_input_ids['images'][0].to(model.device).to(torch.bfloat16)]] if image is not None else None,\n",
    "}\n",
    "prompt_only_inputs2 = {\n",
    "    \"input_ids\": torch.stack([prompt_build_input_ids['input_ids'], prompt_build_input_ids['input_ids']], dim=0).to(model.device),\n",
    "    \"token_type_ids\": torch.stack([prompt_build_input_ids['token_type_ids'], prompt_build_input_ids['token_type_ids']], dim=0).to(model.device),\n",
    "    \"attention_mask\": torch.stack([prompt_build_input_ids['attention_mask'], prompt_build_input_ids['attention_mask']], dim=0).to(model.device),\n",
    "    \"images\": [[prompt_build_input_ids['images'][0].to(model.device).to(torch.bfloat16)]]*2 if image is not None else None,\n",
    "}\n",
    "gen_kwargs = {\n",
    "    \"max_new_tokens\": 30,\n",
    "    # \"temperature\": 0,\n",
    "    # \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    # \"logits_processor\": [number_logit_processor],\n",
    "    \"do_sample\": False,\n",
    "    # \"num_return_sequences\": 1,\n",
    "    # \"stopping_criteria\": [\n",
    "    #     StringStoppingCriteria(tokenizer, len(input_tokens))\n",
    "    # ],\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.forward(**image_only_inputs1, use_cache=True)\n",
    "# generation_output = model.generate(**image_only_inputs2, **gen_kwargs, return_dict_in_generate=True)\n",
    "# with torch.no_grad():\n",
    "#     output = model.forward(**image_only_inputs, use_cache=True, past_key_values=None)\n",
    "# response = model.generate(**inputs2, **gen_kwargs)\n",
    "response_kv = model.generate(**prompt_only_inputs2, **gen_kwargs, past_key_values=output.past_key_values)\n",
    "# output = model(**inputs)\n",
    "# response_strs = tokenizer.decode(response, skip_special_tokens=True)\n",
    "# logits = output.logits[0, -1]\n",
    "torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2314])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_kv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Describe this person in a poem'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(response_kv[0], skip_special_tokens=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.67s/it]\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.bfloat16\n",
    "llama = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", token=hf_access_token, torch_dtype=dtype).cuda()\n",
    "# llama = AutoModelForCausalLM.from_pretrained(\"models/now\", token=hf_access_token, torch_dtype=dtype).cuda()\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "# tokenizer.pad_token_id = tokenizer.unk_token_id\n",
    "# tokenizer.padding_side = \"left\"\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here is how you can modify the'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generation as usual\n",
    "prompt = \"Question: Please write a function in Python that transforms bytes to Giga bytes.\\n\\nAnswer: Here\"\n",
    "llama_inputs = llama_tokenizer(prompt, return_tensors='pt').to(llama.device)\n",
    "generation_output = llama.generate(**llama_inputs, max_new_tokens=6, return_dict_in_generate=True)\n",
    "decoded_output = llama_tokenizer.batch_decode(generation_output.sequences)[0]\n",
    "\n",
    "# Piping the returned `past_key_values` to speed up the next conversation round\n",
    "prompt = decoded_output + \"\\nQuestion: How can I modify the function above to return Mega bytes instead?\\n\\nAnswer: Here\"\n",
    "llama_inputs = llama_tokenizer(prompt, return_tensors='pt').to(llama.device)\n",
    "generation_output = llama.generate(\n",
    "  **llama_inputs,\n",
    "  past_key_values=generation_output.past_key_values,\n",
    "  max_new_tokens=6,\n",
    "  return_dict_in_generate=True\n",
    ")\n",
    "llama_tokenizer.batch_decode(generation_output.sequences)[0][len(prompt):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
