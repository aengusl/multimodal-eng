{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "# import matplotlib.pyplot as plt\n",
    "import tqdm.notebook as tqdm\n",
    "import random\n",
    "from itertools import cycle\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import tqdm.notebook as tqdm\n",
    "# from peft import PeftModel, PeftConfig, get_peft_model\n",
    "# from datasets import load_dataset\n",
    "# import wandb\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "load_dotenv()\n",
    "hf_access_token = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from dataclasses import asdict\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "exp_time = datetime.now().strftime(\"%b%d\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"THUDM/cogvlm2-llama3-chat-19B-int4\"\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "TORCH_TYPE = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability()[\n",
    "    0] >= 8 else torch.float16\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=TORCH_TYPE,\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch, infer_auto_device_map\n",
    "\n",
    "MODEL_PATH = \"THUDM/cogvlm2-llama3-chat-19B\"\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "TORCH_TYPE = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability()[\n",
    "    0] >= 8 else torch.float16\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "with init_empty_weights():\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        torch_dtype=TORCH_TYPE,\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "\n",
    "device_map = infer_auto_device_map(\n",
    "    model=model,\n",
    "    max_memory={i: \"23GiB\" for i in range(torch.cuda.device_count())},\n",
    "    # set 23GiB for each GPU, depends on your GPU memory, you can adjust this value\n",
    "    no_split_module_classes=[\"CogVLMDecoderLayer\"]\n",
    ")\n",
    "\n",
    "save_dir = \"/root/multimodal-eng/models/3jun-int4\"\n",
    "model.save_pretrained(save_dir)\n",
    "checkpoint = \"/root/multimodal-eng/models/3jun-int4\"\n",
    "# model = load_checkpoint_and_dispatch(model, MODEL_PATH, device_map=device_map, dtype=TORCH_TYPE)\n",
    "model = load_checkpoint_and_dispatch(model, checkpoint, device_map=device_map, dtype=TORCH_TYPE)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_PATH = \"THUDM/cogvlm2-llama3-chat-19B\"\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "TORCH_TYPE = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8 else torch.float16\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=TORCH_TYPE,\n",
    "    trust_remote_code=True,\n",
    ").to(DEVICE).eval()\n",
    "\n",
    "text_only_template = \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: {} ASSISTANT:\"\n",
    "\n",
    "while True:\n",
    "    image_path = input(\"image path >>>>> \")\n",
    "    if image_path == '':\n",
    "        print('You did not enter image path, the following will be a plain text conversation.')\n",
    "        image = None\n",
    "        text_only_first_query = True\n",
    "    else:\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "    history = []\n",
    "\n",
    "    while True:\n",
    "        query = input(\"Human:\")\n",
    "        if query == \"clear\":\n",
    "            break\n",
    "\n",
    "        if image is None:\n",
    "            if text_only_first_query:\n",
    "                query = text_only_template.format(query)\n",
    "                text_only_first_query = False\n",
    "            else:\n",
    "                old_prompt = ''\n",
    "                for _, (old_query, response) in enumerate(history):\n",
    "                    old_prompt += old_query + \" \" + response + \"\\n\"\n",
    "                query = old_prompt + \"USER: {} ASSISTANT:\".format(query)\n",
    "        if image is None:\n",
    "            input_by_model = model.build_conversation_input_ids(\n",
    "                tokenizer,\n",
    "                query=query,\n",
    "                history=history,\n",
    "                template_version='chat'\n",
    "            )\n",
    "        else:\n",
    "            input_by_model = model.build_conversation_input_ids(\n",
    "                tokenizer,\n",
    "                query=query,\n",
    "                history=history,\n",
    "                images=[image],\n",
    "                template_version='chat'\n",
    "            )\n",
    "        inputs = {\n",
    "            'input_ids': input_by_model['input_ids'].unsqueeze(0).to(DEVICE),\n",
    "            'token_type_ids': input_by_model['token_type_ids'].unsqueeze(0).to(DEVICE),\n",
    "            'attention_mask': input_by_model['attention_mask'].unsqueeze(0).to(DEVICE),\n",
    "            'images': [[input_by_model['images'][0].to(DEVICE).to(TORCH_TYPE)]] if image is not None else None,\n",
    "        }\n",
    "        gen_kwargs = {\n",
    "            \"max_new_tokens\": 2048,\n",
    "            \"pad_token_id\": 128002,  \n",
    "        }\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, **gen_kwargs)\n",
    "            outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "            response = tokenizer.decode(outputs[0])\n",
    "            response = response.split(\"<|end_of_text|>\")[0]\n",
    "            print(\"\\nCogVLM2:\", response)\n",
    "        history.append((query, response))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# getting batched inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is a demo for using CogVLM2 in CLI using multi-GPU with lower memory.\n",
    "If your single GPU is not enough to drive this model, you can use this demo to run this model on multiple graphics cards with limited video memory.\n",
    "Here, we default that your graphics card has 24GB of video memory, which is not enough to load the FP16 / BF16 model.\n",
    "so , need to use two graphics cards to load. We set '23GiB' for each GPU to avoid out of memory.\n",
    "\"\"\"\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch, infer_auto_device_map\n",
    "import os\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from tqdm import tqdm\n",
    "import textwrap\n",
    "\n",
    "MODEL_PATH = \"THUDM/cogvlm2-llama3-chat-19B\"\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "TORCH_TYPE = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability()[\n",
    "    0] >= 8 else torch.float16\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "with init_empty_weights():\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        torch_dtype=TORCH_TYPE,\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "\n",
    "device_map = infer_auto_device_map(\n",
    "    model=model,\n",
    "    max_memory={i: \"23GiB\" for i in range(torch.cuda.device_count())},\n",
    "    # set 23GiB for each GPU, depends on your GPU memory, you can adjust this value\n",
    "    no_split_module_classes=[\"CogVLMDecoderLayer\"]\n",
    ")\n",
    "checkpoint = \"/root/multimodal-eng/models/28may-1\"\n",
    "model = load_checkpoint_and_dispatch(model, checkpoint, device_map=device_map, dtype=TORCH_TYPE)\n",
    "model = model.eval()\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "text_only_template = \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: {} ASSISTANT:\"\n",
    "\n",
    "image_folder = \"/root/multimodal-eng/pics_of_mates_faces/\"\n",
    "user_input = \"Describe this person to me.\"\n",
    "\n",
    "images_and_responses = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_inputs = []\n",
    "\n",
    "for idx, image_file in tqdm(enumerate(os.listdir(image_folder))):\n",
    "    if not image_file.endswith(\".png\"):\n",
    "        continue\n",
    "    image_path = os.path.join(image_folder, image_file)\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "    history = []\n",
    "\n",
    "    input_by_model = model.build_conversation_input_ids(\n",
    "        tokenizer,\n",
    "        query=user_input,\n",
    "        images=[image],\n",
    "        template_version='chat'\n",
    "    )\n",
    "    inputs = {\n",
    "        'input_ids': input_by_model['input_ids'].unsqueeze(0).to(DEVICE),\n",
    "        'token_type_ids': input_by_model['token_type_ids'].unsqueeze(0).to(DEVICE),\n",
    "        'attention_mask': input_by_model['attention_mask'].unsqueeze(0).to(DEVICE),\n",
    "        'images': [[input_by_model['images'][0].to(DEVICE).to(TORCH_TYPE)]] if image is not None else None,\n",
    "    }\n",
    "\n",
    "    batched_inputs.append(inputs)\n",
    "\n",
    "batched_input_ids = torch.cat([inputs['input_ids'] for inputs in batched_inputs], dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids=batched_input_ids[:4, :],\n",
    "        token_type_ids=torch.cat([inputs['token_type_ids'] for inputs in batched_inputs[:4]], dim=0),\n",
    "        max_new_tokens=2048,\n",
    "        pad_token_id=128002,\n",
    "        images=[inputs['images'][0] for inputs in batched_inputs[:4]]\n",
    "    )\n",
    "for output_idx, outputs in enumerate(outputs):\n",
    "    response = tokenizer.decode(outputs)\n",
    "    response = response.split(\"<|end_of_text|>\")[0].split(\"Question:\")[-1]\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jsonformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jsonformer.format import highlight_values\n",
    "from jsonformer.main import Jsonformer\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(\"Loading model and tokenizer...\")\n",
    "model_name = \"databricks/dolly-v2-3b\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, use_cache=True, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, use_cache=True)\n",
    "print(\"Loaded model and tokenizer\")\n",
    "\n",
    "car = {\n",
    "  \"type\": \"object\",\n",
    "  \"properties\": {\n",
    "    \"car\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"make\": {\"type\": \"string\"},\n",
    "        \"model\": {\"type\": \"string\"},\n",
    "        \"year\": {\"type\": \"number\"},\n",
    "        \"colors\": {\n",
    "          \"type\": \"array\",\n",
    "          \"items\": {\"type\": \"string\"}\n",
    "        },\n",
    "        \"features\": {\n",
    "          \"type\": \"object\",\n",
    "          \"properties\": {\n",
    "            \"audio\": {\n",
    "              \"type\": \"object\",\n",
    "              \"properties\": {\n",
    "                \"brand\": {\"type\": \"string\"},\n",
    "                \"speakers\": {\"type\": \"number\"},\n",
    "                \"hasBluetooth\": {\"type\": \"boolean\"}\n",
    "              }\n",
    "            },\n",
    "            \"safety\": {\n",
    "              \"type\": \"object\",\n",
    "              \"properties\": {\n",
    "                \"airbags\": {\"type\": \"number\"},\n",
    "                \"parkingSensors\": {\"type\": \"boolean\"},\n",
    "                \"laneAssist\": {\"type\": \"boolean\"}\n",
    "              }\n",
    "            },\n",
    "            \"performance\": {\n",
    "              \"type\": \"object\",\n",
    "              \"properties\": {\n",
    "                \"engine\": {\"type\": \"string\"},\n",
    "                \"horsepower\": {\"type\": \"number\"},\n",
    "                \"topSpeed\": {\"type\": \"number\"}\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    \"owner\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"firstName\": {\"type\": \"string\"},\n",
    "        \"lastName\": {\"type\": \"string\"},\n",
    "        \"age\": {\"type\": \"number\"},\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "builder = Jsonformer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    json_schema=car,\n",
    "    prompt=\"Generate an example car\",\n",
    ")\n",
    "\n",
    "print(\"Generating...\")\n",
    "output = builder()\n",
    "\n",
    "highlight_values(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.paddding_side = \"left\"\n",
    "\n",
    "while True:\n",
    "    text_input = input(\"Enter a question: \")\n",
    "    input1 = \"Question: \" + text_input + \"Short Answer:\"\n",
    "    input2 = \"Question: \" + text_input + \"Long Answer:\"\n",
    "    if text_input == \"exit\":\n",
    "        break\n",
    "    batch_tokens = tokenizer([input1, input2], return_tensors=\"pt\")\n",
    "    out = model.generate(**batch_tokens)\n",
    "    print(f\"Short Answer: {tokenizer.decode(out[0])}\")\n",
    "    print(f\"Long Answer: {tokenizer.decode(out[1])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is a demo for using CogVLM2 in CLI using multi-GPU with lower memory.\n",
    "If your single GPU is not enough to drive this model, you can use this demo to run this model on multiple graphics cards with limited video memory.\n",
    "Here, we default that your graphics card has 24GB of video memory, which is not enough to load the FP16 / BF16 model.\n",
    "so , need to use two graphics cards to load. We set '23GiB' for each GPU to avoid out of memory.\n",
    "\"\"\"\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch, infer_auto_device_map\n",
    "import os\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from tqdm import tqdm\n",
    "import textwrap\n",
    "\n",
    "MODEL_PATH = \"THUDM/cogvlm2-llama3-chat-19B\"\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "TORCH_TYPE = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability()[\n",
    "    0] >= 8 else torch.float16\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "with init_empty_weights():\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        torch_dtype=TORCH_TYPE,\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "\n",
    "device_map = infer_auto_device_map(\n",
    "    model=model,\n",
    "    max_memory={i: \"23GiB\" for i in range(torch.cuda.device_count())},\n",
    "    # set 23GiB for each GPU, depends on your GPU memory, you can adjust this value\n",
    "    no_split_module_classes=[\"CogVLMDecoderLayer\"]\n",
    ")\n",
    "checkpoint = \"/root/multimodal-eng/models/28may-1\"\n",
    "model = load_checkpoint_and_dispatch(model, checkpoint, device_map=device_map, dtype=TORCH_TYPE)\n",
    "model = model.eval()\n",
    "print(\"Model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "car = {\n",
    "  \"type\": \"object\",\n",
    "  \"properties\": {\n",
    "    \"car\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"make\": {\"type\": \"string\"},\n",
    "        \"model\": {\"type\": \"string\"},\n",
    "        \"year\": {\"type\": \"number\"},\n",
    "        \"colors\": {\n",
    "          \"type\": \"array\",\n",
    "          \"items\": {\"type\": \"string\"}\n",
    "        },\n",
    "        \"features\": {\n",
    "          \"type\": \"object\",\n",
    "          \"properties\": {\n",
    "            \"audio\": {\n",
    "              \"type\": \"object\",\n",
    "              \"properties\": {\n",
    "                \"brand\": {\"type\": \"string\"},\n",
    "                \"speakers\": {\"type\": \"number\"},\n",
    "                \"hasBluetooth\": {\"type\": \"boolean\"}\n",
    "              }\n",
    "            },\n",
    "            \"safety\": {\n",
    "              \"type\": \"object\",\n",
    "              \"properties\": {\n",
    "                \"airbags\": {\"type\": \"number\"},\n",
    "                \"parkingSensors\": {\"type\": \"boolean\"},\n",
    "                \"laneAssist\": {\"type\": \"boolean\"}\n",
    "              }\n",
    "            },\n",
    "            \"performance\": {\n",
    "              \"type\": \"object\",\n",
    "              \"properties\": {\n",
    "                \"engine\": {\"type\": \"string\"},\n",
    "                \"horsepower\": {\"type\": \"number\"},\n",
    "                \"topSpeed\": {\"type\": \"number\"}\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    \"owner\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"firstName\": {\"type\": \"string\"},\n",
    "        \"lastName\": {\"type\": \"string\"},\n",
    "        \"age\": {\"type\": \"number\"},\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "builder = Jsonformer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    json_schema=car,\n",
    "    prompt=\"Generate an example car\",\n",
    ")\n",
    "\n",
    "print(\"Generating...\")\n",
    "output = builder()\n",
    "\n",
    "highlight_values(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CogVLMJsonformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.venv/multi/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This is a demo for using CogVLM2 in CLI using multi-GPU with lower memory.\n",
    "If your single GPU is not enough to drive this model, you can use this demo to run this model on multiple graphics cards with limited video memory.\n",
    "Here, we default that your graphics card has 24GB of video memory, which is not enough to load the FP16 / BF16 model.\n",
    "so , need to use two graphics cards to load. We set '23GiB' for each GPU to avoid out of memory.\n",
    "\"\"\"\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch, infer_auto_device_map\n",
    "import os\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from tqdm import tqdm\n",
    "import textwrap\n",
    "from jsonformer.format import highlight_values\n",
    "from jsonformer.main import Jsonformer\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from subclass_jsonformer import CogVLMJsonformer\n",
    "\n",
    "\n",
    "\n",
    "car = {\n",
    "  \"type\": \"object\",\n",
    "  \"properties\": {\n",
    "    \"car\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"make\": {\"type\": \"string\"},\n",
    "        \"model\": {\"type\": \"string\"},\n",
    "        \"year\": {\"type\": \"number\"},\n",
    "        \"colors\": {\n",
    "          \"type\": \"array\",\n",
    "          \"items\": {\"type\": \"string\"}\n",
    "        },\n",
    "        \"features\": {\n",
    "          \"type\": \"object\",\n",
    "          \"properties\": {\n",
    "            \"audio\": {\n",
    "              \"type\": \"object\",\n",
    "              \"properties\": {\n",
    "                \"brand\": {\"type\": \"string\"},\n",
    "                \"speakers\": {\"type\": \"number\"},\n",
    "                \"hasBluetooth\": {\"type\": \"boolean\"}\n",
    "              }\n",
    "            },\n",
    "            \"safety\": {\n",
    "              \"type\": \"object\",\n",
    "              \"properties\": {\n",
    "                \"airbags\": {\"type\": \"number\"},\n",
    "                \"parkingSensors\": {\"type\": \"boolean\"},\n",
    "                \"laneAssist\": {\"type\": \"boolean\"}\n",
    "              }\n",
    "            },\n",
    "            \"performance\": {\n",
    "              \"type\": \"object\",\n",
    "              \"properties\": {\n",
    "                \"engine\": {\"type\": \"string\"},\n",
    "                \"horsepower\": {\"type\": \"number\"},\n",
    "                \"topSpeed\": {\"type\": \"number\"}\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    \"owner\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"firstName\": {\"type\": \"string\"},\n",
    "        \"lastName\": {\"type\": \"string\"},\n",
    "        \"age\": {\"type\": \"number\"},\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "MODEL_PATH = \"THUDM/cogvlm2-llama3-chat-19B\"\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "TORCH_TYPE = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability()[\n",
    "    0] >= 8 else torch.float16\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "with init_empty_weights():\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        torch_dtype=TORCH_TYPE,\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "\n",
    "device_map = infer_auto_device_map(\n",
    "    model=model,\n",
    "    max_memory={i: \"23GiB\" for i in range(torch.cuda.device_count())},\n",
    "    # set 23GiB for each GPU, depends on your GPU memory, you can adjust this value\n",
    "    no_split_module_classes=[\"CogVLMDecoderLayer\"]\n",
    ")\n",
    "checkpoint = \"/root/multimodal-eng/models/28may-1\"\n",
    "model = load_checkpoint_and_dispatch(model, checkpoint, device_map=device_map, dtype=TORCH_TYPE)\n",
    "model = model.eval()\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "\n",
    "# builder = Jsonformer(\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"THUDM/cogvlm2-llama3-chat-19B-int4\"\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "TORCH_TYPE = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability()[\n",
    "    0] >= 8 else torch.float16\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=TORCH_TYPE,\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from PIL import Image\n",
    "image = Image.open(\"/root/multimodal-eng/pics_of_mates_faces/Screenshot_2020_04_06_at_151459__.png\").convert('RGB')\n",
    "\n",
    "builder = CogVLMJsonformer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    json_schema=car,\n",
    "    prompt=\"Generate an example car\",\n",
    "    images=[image],\n",
    "    debug=True,\n",
    ")\n",
    "\n",
    "print(\"Generating...\")\n",
    "output = builder()\n",
    "\n",
    "highlight_values(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.venv/multi/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/root/.venv/multi/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "prompt = \"- problems: numbers suck, speed is slow, need batching with images and language stuff, OOMs for image inputs with the 4 bit model, but in principle I can run inference on this model with one gpu, maybe I should debug with this model on the 80gb a100, except even on an a100 I get errors! What happens when I only run one inference step with an image text pair, I might as well get super familiar with how to do this. \"*10\n",
    "prompt12 = \"Describe this person in a poem\"\n",
    "prompt_empty = \"\"\n",
    "\n",
    "image = Image.open(\"/root/multimodal-eng/pics_of_mates_faces/Screenshot_2020_04_06_at_151459__.png\").convert('RGB')\n",
    "\n",
    "build_input_ids = model.build_conversation_input_ids(\n",
    "    tokenizer=tokenizer,\n",
    "    query=prompt_empty,\n",
    "    images=[image] if image is not None else None,\n",
    "    template_version=\"base\",\n",
    ")\n",
    "build_input_ids12 = model.build_conversation_input_ids(\n",
    "    tokenizer=tokenizer,\n",
    "    query=prompt12,\n",
    "    images=[image] if image is not None else None,\n",
    "    template_version=\"base\",\n",
    ")\n",
    "input_tokens = build_input_ids['input_ids'].to(model.device)\n",
    "image_only_inputs = {\n",
    "    'input_ids': build_input_ids['input_ids'].unsqueeze(0).to(model.device),\n",
    "    'token_type_ids': build_input_ids['token_type_ids'].unsqueeze(0).to(model.device),\n",
    "    'attention_mask': build_input_ids['attention_mask'].unsqueeze(0).to(model.device),\n",
    "    'images': [[build_input_ids['images'][0].to(model.device).to(torch.bfloat16)]] if image is not None else None,\n",
    "}\n",
    "inputs1 = {\n",
    "    'input_ids': build_input_ids12['input_ids'].unsqueeze(0).to(model.device),\n",
    "    'token_type_ids': build_input_ids12['token_type_ids'].unsqueeze(0).to(model.device),\n",
    "    'attention_mask': build_input_ids12['attention_mask'].unsqueeze(0).to(model.device),\n",
    "    'images': [[build_input_ids12['images'][0].to(model.device).to(torch.bfloat16)]] if image is not None else None,\n",
    "}\n",
    "inputs2 = {\n",
    "    \"input_ids\": torch.stack([build_input_ids12['input_ids'], build_input_ids12['input_ids']], dim=0).to(model.device),\n",
    "    \"token_type_ids\": torch.stack([build_input_ids12['token_type_ids'], build_input_ids12['token_type_ids']], dim=0).to(model.device),\n",
    "    \"attention_mask\": torch.stack([build_input_ids12['attention_mask'], build_input_ids12['attention_mask']], dim=0).to(model.device),\n",
    "    \"images\": [[build_input_ids12['images'][0].to(model.device).to(torch.bfloat16)]]*2 if image is not None else None,\n",
    "}\n",
    "gen_kwargs = {\n",
    "    \"max_new_tokens\": 30,\n",
    "    # \"temperature\": 0,\n",
    "    # \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    # \"logits_processor\": [number_logit_processor],\n",
    "    \"do_sample\": False,\n",
    "    # \"num_return_sequences\": 1,\n",
    "    # \"stopping_criteria\": [\n",
    "    #     StringStoppingCriteria(tokenizer, len(input_tokens))\n",
    "    # ],\n",
    "}\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     output = model.forward(**image_only_inputs, use_cache=True)\n",
    "generation_output = model.generate(**image_only_inputs, **gen_kwargs, return_dict_in_generate=True)\n",
    "response = model.generate(**inputs1, **gen_kwargs)\n",
    "response_kv = model.generate(**inputs1, **gen_kwargs, past_key_values=generation_output.past_key_values, return_dict_in_generate=True)\n",
    "# output = model(**inputs)\n",
    "# response_strs = tokenizer.decode(response, skip_special_tokens=True)\n",
    "# logits = output.logits[0, -1]\n",
    "torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Describe this person in a poem\\n\\nIn a world of fleeting moments,\\nA face emerges, bold and unblunted.\\nWith eyes that pierce, and lips that part,\\nA',\n",
       " 'Describe this person in a poem')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(response[0], skip_special_tokens=True), tokenizer.decode(response_kv.sequences[0], skip_special_tokens=True)\n",
    "# response.shape, response_kv.sequences.shape\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.venv/multi/lib/python3.10/site-packages/transformers/generation/utils.py:1659: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "CogVLMForCausalLM.prepare_inputs_for_generation() missing 1 required positional argument: 'token_type_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion: Please write a function in Python that transforms bytes to Giga bytes.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAnswer: Here\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m tokenizer(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m generation_output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m decoded_output \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(generation_output\u001b[38;5;241m.\u001b[39msequences)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Piping the returned `past_key_values` to speed up the next conversation round\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/multi/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/multi/lib/python3.10/site-packages/transformers/generation/utils.py:1758\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1750\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1751\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1752\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1753\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1754\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1755\u001b[0m     )\n\u001b[1;32m   1757\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 1758\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1759\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1760\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1761\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1762\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1763\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1764\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1765\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1766\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1767\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1769\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   1770\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1771\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1772\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config) \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1773\u001b[0m     )\n",
      "File \u001b[0;32m~/.venv/multi/lib/python3.10/site-packages/transformers/generation/utils.py:2394\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2390\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_initial_cache_position(input_ids, model_kwargs)\n\u001b[1;32m   2392\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_unfinished_sequences(this_peer_finished, synced_gpus, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice):\n\u001b[1;32m   2393\u001b[0m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[0;32m-> 2394\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_inputs_for_generation\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2396\u001b[0m     \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[1;32m   2397\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\n\u001b[1;32m   2398\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs,\n\u001b[1;32m   2399\u001b[0m         return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   2400\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   2401\u001b[0m         output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   2402\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: CogVLMForCausalLM.prepare_inputs_for_generation() missing 1 required positional argument: 'token_type_ids'"
     ]
    }
   ],
   "source": [
    "# Generation as usual\n",
    "prompt = \"Question: Please write a function in Python that transforms bytes to Giga bytes.\\n\\nAnswer: Here\"\n",
    "model_inputs = tokenizer(prompt, return_tensors='pt')\n",
    "generation_output = model.generate(**model_inputs, max_new_tokens=60, return_dict_in_generate=True)\n",
    "decoded_output = tokenizer.batch_decode(generation_output.sequences)[0]\n",
    "\n",
    "# Piping the returned `past_key_values` to speed up the next conversation round\n",
    "prompt = decoded_output + \"\\nQuestion: How can I modify the function above to return Mega bytes instead?\\n\\nAnswer: Here\"\n",
    "model_inputs = tokenizer(prompt, return_tensors='pt')\n",
    "generation_output = model.generate(\n",
    "  **model_inputs,\n",
    "  past_key_values=generation_output.past_key_values,\n",
    "  max_new_tokens=60,\n",
    "  return_dict_in_generate=True\n",
    ")\n",
    "tokenizer.batch_decode(generation_output.sequences)[0][len(prompt):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2343]), torch.Size([2314]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[0].shape, response_kv[0].shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 2307, 128])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.past_key_values[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.generate(**inputs2, **gen_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_strs[0]: Describe this person in a poem\n",
      "\n",
      "In a world of fleeting moments,\n",
      "Stands\n",
      "\n",
      "response_strs[1]: Describe this person in a poem\n",
      "\n",
      "In a moment so fleeting and rare,\n",
      "A\n"
     ]
    }
   ],
   "source": [
    "response_strs = tokenizer.batch_decode(response, skip_special_tokens=True)\n",
    "print(f\"response_strs[0]: {response_strs[0]}\\n\")\n",
    "print(f\"response_strs[1]: {response_strs[1]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2306, device='cuda:0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs2['token_type_ids'][0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
